{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4f6d47",
   "metadata": {},
   "source": [
    "# Feedforward in neural networks\n",
    "\n",
    "In this notebook we will take the first step towards desigining and training neural networks. We'll start slowly by writing code to implement the feedforward step with weights and biases fixed. In the next notebook, we'll take on the challenge of training the network (i.e. optimizing the weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "60263097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f820d",
   "metadata": {},
   "source": [
    "## Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903415f1",
   "metadata": {},
   "source": [
    "Let's design a network with:\n",
    "- an input layer with 3 neurons\n",
    "- a hidden layer with 2 neurons\n",
    "- an output layer with 2 neurons\n",
    "\n",
    "<img src=\"NN.png\"  width=\"600\" height=\"300\">\n",
    "\n",
    "We therefore need two weights matrices, $w^2$ and $w^3$, and two bias vectors $b^2$ and $b^3$. Note that the input layer (layer 1) isn't really a layer of neurons and so doesn't need any weights or biases. Let's set up an example input $\\mathbf{x}$ to start with. The vector $\\mathbf{x}$ has three components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84020405",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,0,-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc83bfbc",
   "metadata": {},
   "source": [
    "Let's also pick some weights and biases. We'll just make up some numbers here as we are not yet training our network, just computing the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "969be6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = np.array(\n",
    "    [[1,2,1],[-1,-2,0]]\n",
    ")\n",
    "w3 = np.array(\n",
    "    [[1,-1],[2,2]]\n",
    ")\n",
    "b2 = np.array([2,-1])\n",
    "b3 = np.array([1,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e0f8c",
   "metadata": {},
   "source": [
    "Make sure you understand in this simple example what the entries in these matrices mean. As a test just for yourself (no need to write this down), find the weight which is multiplied by the first input (`x[0]`) in the second neuron of the second layer.\n",
    "\n",
    "To get the activations for the second layer (the hidden layer), we need to use the formula\n",
    "$$\n",
    "a^2 = \\sigma(w^1 x+b^1) \n",
    "$$\n",
    "Let's define the sigmoid now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "75a4fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c46e8",
   "metadata": {},
   "source": [
    "Now we can compute the activations in the second (hidden) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e5914dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = sigmoid(w2 @ x + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "478a4a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.11920292])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb91805",
   "metadata": {},
   "source": [
    "This is a vector of size two, as it should be -- one number for each neuron in the hidden layer. Now let's compute the output from the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a5e26e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = sigmoid(w3 @ a2 + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ea4f2929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83366886, 0.66830372])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ab583",
   "metadata": {},
   "source": [
    "The activations in the final layer are also known as the _output_ of our neural network. So far we've done everything fairly carefully and by hand. If we want our code to work for larger networks, we'll want to use a `for` loop. Let's first make two lists, one of weights and one of biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f6f544ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [w2, w3]\n",
    "b = [b2, b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f58188",
   "metadata": {},
   "source": [
    "To perform the feedforward, we loop through each non-input layer and compute the activation. We'll use a single variable `a` to track the activations, and use `zip()` to put the weights and biases into a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8c2b28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(weights, biases, x):\n",
    "    a = x\n",
    "    for w, b in zip(weights, biases):\n",
    "        a = sigmoid(w @ a + b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227491d1",
   "metadata": {},
   "source": [
    "Let's make sure it agrees with our slow computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1aeffac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83366886, 0.66830372])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward(w,b,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9cdf9",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "Now let's introduce some training data. We're still not going to actually train our network, but we'll compute a cost function and see what happens. Our training data needs to consist of _training inputs_ and training labels or _targets_. We'll use a hypothetical classification problem on 3D data. Our classification consists of two classes: class A is for vectors with norm at most 1, and class B is for vectors with norm more than 1. \n",
    "\n",
    "As with any classification problem (e.g. MNIST), we need to encode a classification as an output vector. We'll use $[1,0]$ for class A, $[0,1]$ for class B. More generally, for an output $[a,b]$, $a$ encodes the confidence for class A and $b$ encodes the confidence in class B.\n",
    "\n",
    "Let's build our training data. We'll generate ten random vectors with coordinates between $-1$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f772f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "X = np.random.uniform(-1,1,size=(10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "29f65acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35602339,  0.7808449 ,  0.17610451],\n",
       "       [-0.74680781, -0.71731755, -0.06420882],\n",
       "       [-0.95582068,  0.45454943,  0.04877468],\n",
       "       [ 0.08987048, -0.08725348,  0.00276453],\n",
       "       [-0.21106289, -0.69765539, -0.27824965],\n",
       "       [-0.67584599, -0.32408261, -0.63935344],\n",
       "       [-0.2180172 , -0.92870358,  0.1297233 ],\n",
       "       [-0.59307702, -0.35879108, -0.24687243],\n",
       "       [-0.63189172, -0.79209633, -0.09014555],\n",
       "       [-0.60827233, -0.24294915,  0.86106392]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45f8865",
   "metadata": {},
   "source": [
    "Each row is a data point, and we need to compute a ground truth label for it (that is, a binary vector). We'll start with an empty list and add either $[1,0]$ or $[0,1]$ to it as we go through our training inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83d48a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for x in X:\n",
    "    if np.linalg.norm(x) <= 1:\n",
    "        y.append(np.array([1,0]))\n",
    "    else:\n",
    "        y.append(np.array([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "82283302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 0]),\n",
       " array([0, 1]),\n",
       " array([0, 1]),\n",
       " array([1, 0]),\n",
       " array([1, 0]),\n",
       " array([1, 0]),\n",
       " array([1, 0]),\n",
       " array([1, 0]),\n",
       " array([0, 1]),\n",
       " array([0, 1])]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e4e2b",
   "metadata": {},
   "source": [
    "Note that usually the training data is given to you. The only reason we had to generate it here was because we are generating an example ourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227b401",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "We just saw how to compute the output for a _single_ input. Now let's compute the outputs for every training input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3280fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = [\n",
    "    feedforward(w, b, X[i]) for i in range(len(X))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "59486e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.86621361, 0.75634521]),\n",
       " array([0.66232822, 0.80348226]),\n",
       " array([0.83240746, 0.7889282 ]),\n",
       " array([0.83005625, 0.78845977]),\n",
       " array([0.70717419, 0.79443203]),\n",
       " array([0.7168432 , 0.76475332]),\n",
       " array([0.68307604, 0.82030231]),\n",
       " array([0.73726489, 0.79766862]),\n",
       " array([0.65760997, 0.80072279]),\n",
       " array([0.79093087, 0.85260651])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50bcf82",
   "metadata": {},
   "source": [
    "As you can see, we get a list of 2D arrays. How do we determine the level of accuracy of this output? We need to decide, for each output, what the prediction actually means. A simple way to do this is to look at the output vector and record the highest value. So if the first coordinate is higher, we interpret that as \"class A\" and if the second is higher we interpret that as \"class B\". Let's record this classification in a vector called `predicted`, and record the real answer as `truth`. Make sure you understand what these `for` loops are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "02b5b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "for v in all_outputs:\n",
    "    if v[0] > v[1]:\n",
    "        predicted.append('A')\n",
    "    else:\n",
    "        predicted.append('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a547460d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1d443541",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = []\n",
    "for v in y:\n",
    "    if v[0] == 1:\n",
    "        truth.append('A')\n",
    "    else:\n",
    "        truth.append('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "12e7593e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'B']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a3dd1c",
   "metadata": {},
   "source": [
    "To find our accuracy, we can make a vector recording whether `predicted` and `truth` agree, count the truth values, and divide by the total number of training inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4874049b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False,  True, False, False, False, False,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(truth) == np.array(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fa5c5b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(truth) == np.array(predicted))/len(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97956415",
   "metadata": {},
   "source": [
    "Looks like our badly chosen weights and biases got it right about 50% of the time. This is about as good as the sophisticated machine learning algorithm called \"flipping a coin\". This is an important lesson: neural networks are not good because we can compute their output, but because we can _train_ them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5748ee11",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "Now we'll define our cost function. Our function `cost` has to do the following _for each training input_: (1) compute the output of the network and (2) compute the mean squared error using the training labels. We then add all of these values together and divide by $2n$, where $n$ is the number of training inputs. See the Exercise below.\n",
    "\n",
    "**Note:** We could theoretically use accuracy as our cost function, but then we would not be able to take any derivatives. We hope that our cost function does a good job of approximating accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d7fb3f",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "(a) Fill in the ... below to complete the `cost` function. If you want to see the formula for the cost, check out Equation (6) in Nielsen. \n",
    "\n",
    "(b) Run the cost function using the weights and biases we picked before and the training data `X` and training labels `y`. You should get an answer of about `0.3` or so.\n",
    "\n",
    "(c) Compute the cost again, but this time use the following weights and biases which I obtained by optimizing the network a bit (what you'll learn to do in a later notebook):\n",
    "\n",
    "$$\n",
    "w^2 = \\begin{pmatrix}\n",
    "1.3 & -0.3 & -1.4 \\\\\n",
    "2.1 & 0.7 & -1.8\n",
    "\\end{pmatrix} \\ w^3 = \\begin{pmatrix}\n",
    "0.1 & 1.2 \\\\ -0.9 & -2.3 \n",
    "\\end{pmatrix}\\ b^2 = \\begin{pmatrix} 0.1 \\\\ 1.0 \\end{pmatrix} \\ b^3 = \\begin{pmatrix} -0.1 \\\\ 1.0 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "(d) Compute the accuracy for these new weights and biases. Did the neural network do better or worse on the training data with these new weights?\n",
    "\n",
    "(e) Compute the output of the network with the weights and biases in (c) when the input is the vector $[100,100,100]^\\mathsf{T}$. Based on the output, does the network think this vector has norm less than or greater than $1$? Does the answer surprise you?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2845fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part a\n",
    "def cost(weights, biases, training_input, training_labels):\n",
    "    total_cost = 0\n",
    "    n = len(training_input)\n",
    "    for i in range(len(training_input)):\n",
    "        output = feedforward(weights, biases, training_input[i]) #compute feedforward for the ith training input\n",
    "        cost = (np.linalg.norm(output - training_labels[i]))**2 #compute the square of the norm of the difference between the output and the ith training label\n",
    "        total_cost = total_cost + cost\n",
    "        \n",
    "    return total_cost/(2 * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "44221d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32176264964887974"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part b\n",
    "cost(w, b, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "da03c86e-65ca-4124-a0e6-24d75af52537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17275998032344547"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part c\n",
    "v2 = np.array(\n",
    "    [[1.3,-0.3,-1.4],[2.1,0.7,-1.8]]\n",
    ")\n",
    "v3 = np.array(\n",
    "    [[0.1,1.2],[-0.9,-2.3]]\n",
    ")\n",
    "u2 = np.array([0.1,1])\n",
    "u3 = np.array([-0.1,1])\n",
    "v = [v2, v3]\n",
    "u = [u2, u3]\n",
    "cost(v, u, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fa75af15-f5e6-473b-9ef8-da8fd71d26e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.66193477, 0.33346052]),\n",
       " array([0.56702314, 0.50870135]),\n",
       " array([0.57410903, 0.522435  ]),\n",
       " array([0.70293876, 0.22463358]),\n",
       " array([0.6740951 , 0.26642097]),\n",
       " array([0.66898526, 0.28249864]),\n",
       " array([0.60978181, 0.40464942]),\n",
       " array([0.62922836, 0.37257945]),\n",
       " array([0.58279122, 0.46908921]),\n",
       " array([0.51431626, 0.64567114])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part d\n",
    "all_outputs2 = [\n",
    "    feedforward(v, u, X[i]) for i in range(len(X))\n",
    "]\n",
    "all_outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9788d8bf-8f73-4a83-b963-6d9df3324d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted2 = []\n",
    "for m in all_outputs2:\n",
    "    if m[0] > m[1]:\n",
    "        predicted2.append('A')\n",
    "    else:\n",
    "        predicted2.append('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b204965f-f0f8-404a-b894-ef280ce5e816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(truth) == np.array(predicted2))/len(truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d48894-ed74-450c-aabb-16982fcad129",
   "metadata": {},
   "source": [
    "Seems these weight and biases performed better by 20%. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2be7a3d0-7af8-48d0-8071-8bb0c51b837a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026011, 0.21416502])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part e\n",
    "training_input = np.array([100, 100, 100])\n",
    "output = feedforward(v, u, training_input) \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "99a241d6-8b68-4bb5-a99a-d65ea081e751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173.20508075688772"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.array([100, 100, 100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74ab56f1-5c1b-4749-b7ea-2755262065d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.69515757e-18 1.00000000e+00]\n",
      "[0.75026011 0.21416502]\n"
     ]
    }
   ],
   "source": [
    "a2 = sigmoid(v2 @ training_input + u2)\n",
    "a3 = sigmoid(v3 @ a2 + u3)\n",
    "print(a2)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e6dcf-4cc4-49ea-a4e0-6120b2a59ef4",
   "metadata": {},
   "source": [
    "The network thinks [100, 100, 100]^T has a norm less than 1. This is surprising, but when looking at the activations it makes some sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff32423",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "By hand, find weights and biases for this network so that the output answers the following question:\n",
    "\n",
    "_Given an input vector $x$, are the entries of $x$ in ascending order?_\n",
    "\n",
    "Test your network on a few inputs to make sure it's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1cd81f63-fee1-40ae-90c3-76c9699dfd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_ch = np.array(\n",
    "    [[-1, 0, 1],[0, -1, 1]] #difference between the third and first should be greater than difference between second and third\n",
    ")\n",
    "w3_ch = np.array(\n",
    "    [[1.9, 0],[0,1.9]]\n",
    ")\n",
    "b2_ch = np.array([0.01,-0.01])\n",
    "b3_ch = np.array([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ef37aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ch = [w2_ch, w3_ch]\n",
    "b_ch = [b2_ch, b3_ch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2a6d4-7389-451f-8616-38df31030bb4",
   "metadata": {},
   "source": [
    "Recall Truth = ['A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'B'] (I kept with the same pattern, except a [1,0] will mean ascending and [0,1] will mean not ascending. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8676aab9-aef6-4804-9cbf-66f29a959301",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [np.array([0.1, 0.2, 0.3]), np.array([0.52, 0.42, 0.73]), np.array([0.5, 0.2, 0.1]), np.array([-0.1, 0.4, 0.6]), np.array([0, 0.8, 0.9]), \n",
    "        np.array([-0.25, 0.34, 0.61]), np.array([-0.5, -0.4, -0.3]), np.array([-0.97, -0.43, -0.3]), np.array([0.5, 0.2, 0.3]), np.array([0.6, 0.4, 0.3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "05423380-5cc3-4de3-a73f-4c2d1d17ea05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.74065679, 0.72962507]),\n",
       " array([0.74155771, 0.74865293]),\n",
       " array([0.68288519, 0.71049773]),\n",
       " array([0.78138609, 0.73884605]),\n",
       " array([0.79489701, 0.72962507]),\n",
       " array([0.79232821, 0.74513073]),\n",
       " array([0.74065679, 0.72962507]),\n",
       " array([0.77921601, 0.73241925]),\n",
       " array([0.70266494, 0.72962507]),\n",
       " array([0.69279255, 0.71049773])]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs3 = [\n",
    "    feedforward(w_ch, b_ch, test[i]) for i in range(len(test))\n",
    "]\n",
    "all_outputs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d884d993-376a-4347-81a7-068c7f8da43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted3 = []\n",
    "for m in all_outputs3:\n",
    "    if m[0] > m[1]:\n",
    "        predicted3.append('A')\n",
    "    else:\n",
    "        predicted3.append('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f9d3aca5-038c-41cd-9b0b-f3ec7c206d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(truth) == np.array(predicted3))/len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bff4db-71cb-4fe2-97c2-5a2f8da110f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

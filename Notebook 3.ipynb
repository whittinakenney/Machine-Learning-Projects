{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8146e4b9",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "\n",
    "In this notebook, we'll look at an exciting feature of the powerful `tensorflow` library developed by Google and released as an open source library in 2015. Tensorflow has many machine learning functions, but its most prominent use is to set up and train neural networks. Tensorflow works on multiple operating systems, coding languages and can leverage both CPU and GPU capabilities. \n",
    "\n",
    "In this exercise we'll look at just one of the features of `tensorflow`, namely _automatic differentiation_. To start with, let's import the library. If this doesn't work it may be because you started a Datascience Environment by mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c8c05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d876a",
   "metadata": {},
   "source": [
    "## Python doesn't remember operations\n",
    "\n",
    "Before we look at automatic differentiation with `tensorflow`, let's recall some basic facts about basic Python. Let's make a variable called `x`, store `5` as a value, and then let `y` be the square of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2490705",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5\n",
    "y = x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f4c97",
   "metadata": {},
   "source": [
    "If we look at what `y` is, we'll just get the value `25`. Nothing more. We can't ask for $\\frac{dy}{dx}$ because `y` is just a number that happens to have the value `25`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0aa347a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe97bc8",
   "metadata": {},
   "source": [
    "The key feature of `tensorflow` is that it remembers operations, so it can compute the gradient of one variable with respect to another. Let's try that out now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f668c2",
   "metadata": {},
   "source": [
    "## Univariate functions with `tensorflow`\n",
    "\n",
    "Let's try what we did above again with `tensorflow`. The Golden Rule is: for automatic differentiation to work, you **have to do all operations inside tensorflow**. This means that your variables have to be `tensorflow` and, in theory, your operations have to be `tensorflow` operations. An exception to this is that some basic operations like multiply, add etc., are already automatically done with `tensorflow`. \n",
    "\n",
    "Let's define our variable `x` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef904af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005af220",
   "metadata": {},
   "source": [
    "This looks okay, but there's an issue. Because we put `5` as the value, `tensorflow` is going to assume this variable is an integer, which will be an issue later when we want to differentiate. Indeed, let's see what happens when we look at `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c7f0016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=5>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96847f",
   "metadata": {},
   "source": [
    "See the `dtype=int32`? That's not going to work. To fix this we can either give a `dtype=tf.float32` argument to `tf.Variable`, or we can just make the initial value `5.0` and that will automatically make the number a real number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aaaa4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e157a53",
   "metadata": {},
   "source": [
    "Tensorflow \"records\" operations that are executed inside the context of a gradient \"tape\". This is a little bit complicated to explain fully, but what matters is the coding syntax we use. Let's suppose we want to let `y` be `x` squared, and we want `tensorflow` to track that operation. We do this as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "120f2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87dafca",
   "metadata": {},
   "source": [
    "Let's see what's in `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a077015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=25.0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732f0e5",
   "metadata": {},
   "source": [
    "It has a value of `25` as you can see. But because we tracked the operation, we can now calculate the value of $\\frac{dy}{dx}$ at $x = 5$ with a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3336d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx = tape.gradient(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c7bc242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94438e",
   "metadata": {},
   "source": [
    "Note that we used `tape` to indicate how the operations were recorded. We got the value of `dy_dx` as $10$. Check for yourself that this is the correct value of the derivative of the function $y = x^2$ at $x = 5$. What's amazing is that `tensorflow` knows the Chain Rule, so you can use it to calculate the gradient for complicated functions. Let's do this now, we'll calculate the derivative of the function \n",
    "$$\n",
    "f(x) = \\ln(2(x+1)^3+5x)\n",
    "$$\n",
    "at the value $x = 3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9266df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.math.log(2*(x+1)**3+5*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b16f1b",
   "metadata": {},
   "source": [
    "Notice how we didn't use the `log` function in `numpy` or anywhere else. We have to use the `tensorflow` version for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58998e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.7062937>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx = tape.gradient(y,x)\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352c5d0",
   "metadata": {},
   "source": [
    "You can check this by hand or using WolframAlpha if you like. One technical thing to note is that if you try and run the code block above again (feel free to try it), you'll get an error. As soon as you compute a gradient using a tape, `tensorflow` assumes you're done with that tape and discards it to save memory. This makes sense because `tensorflow` is designed for large scale machine learning computations. You can force it to stick around, though, using a `persistent=True` flag when making the tape.\n",
    "\n",
    "We could also have built up the function $f$ piece by piece and let `tensorflow` do the Chain Rule for us. Hopefully we got the same answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecaf8959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.7062937>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y1 = 2*(x+1)**3 + 5*x\n",
    "    y2 = tf.math.log(y1)\n",
    "dy_dx = tape.gradient(y2,x)\n",
    "dy_dx "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd004029",
   "metadata": {},
   "source": [
    "## Gradients with `tensorflow`\n",
    "\n",
    "Tensorflow loves to work with vectors (and even tensors!), so computing gradients is also simple. The main pitfall to avoid is using `numpy` operations instead of `tensorflow` ones. Let's compute the gradient of the function $f: \\mathbb{R}^3 \\to \\mathbb{R},\\ f(\\mathbf{x}) = ||x||^2$ at $\\mathbf{x} = [1,1,1]^\\mathsf{T}$. We first create a variable as before, but this time it's a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cefb2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([1.0,1.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc40e03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90036adc",
   "metadata": {},
   "source": [
    "Let's set up a tape and calculate the norm of `x`. Remember to use `tensorflow` operations! If you not sure what the right function is in tensorflow, just Google it (that's what I did!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bee00f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = tf.norm(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5011abc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([2., 2., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx = tape.gradient(y,x)\n",
    "dy_dx "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12077b4",
   "metadata": {},
   "source": [
    "We should get an answer of $2x^\\mathsf{T}$, and indeed that's what we get (though `tensorflow` doesn't distinguish between row and column vectors here).\n",
    "\n",
    "Let's check that the chain rule still works but computing the gradient of the function $f(\\mathbf{x}) = ||x||^4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3da09a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([12., 12., 12.], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([1.0,1.0,1.0])\n",
    "with tf.GradientTape() as tape:\n",
    "    y1 = tf.norm(x)\n",
    "    y2 = y1**4\n",
    "dy_dx = tape.gradient(y2,x)\n",
    "dy_dx "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfd5bec",
   "metadata": {},
   "source": [
    "You'll have to finish the Theory Homework to check this one. Finally, note that if you accidentally ask for the gradient of a vector-valued function $f$, `tensorflow` will just take the gradient of each function $f_i$ and add them. When you want a Jacobian, you should use a different function, which we look at next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ed29b3",
   "metadata": {},
   "source": [
    "## Jacobians with `tensorflow`\n",
    "\n",
    "Let's compute the Jacobian of the function $f: \\mathbb{R}^3 \\to \\mathbb{R}^2$ given by \n",
    "$$\n",
    "f(\\mathbf{x}) = A\\mathbf{x} \n",
    "$$\n",
    "where \n",
    "$$\n",
    "A = \\begin{pmatrix} 1 & -1 & 2 \\\\ 5 & 2 & 3\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "If you recall what we did in class, you should get $A$ itself as the Jacobian, no matter what value $\\mathbf{x}$ takes. With that in mind, let's make $\\mathbf{x}$ a completely random vector. We want it to be of size 3, so we give `tf.random.uniform` a single argument of `[3]` to tell it what shape to give us. \n",
    "\n",
    "The matrix `A` is not a variable, it's fixed, so we define it as a `tf.constant`. And we remember to use `tensorflow` operations for the matrix-vector product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ecd2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(tf.random.uniform([3]))\n",
    "A = tf.constant([[1.0,-1.0,2.0], [5.0,2.0,3.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.linalg.matvec(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe0289",
   "metadata": {},
   "source": [
    "Let's see what we got using `tape.jacobian` instead of `gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8236ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1., -1.,  2.],\n",
       "       [ 5.,  2.,  3.]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx = tape.jacobian(y,x)\n",
    "dy_dx "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7473b3",
   "metadata": {},
   "source": [
    "We have barely scratched the surface of automatic differentiation in `tensorflow`. In practice, a lot of this happens in the background when running large neural networks, but it's worth exploring on its own. You don't really need to know the actual base-level implementation of these operations to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dfe0c1",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Use `tensorflow` to find the derivative of the function $y = e^{(x-1)^2}$ at $x = 4$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23ae567f-7433-4c3a-bfa3-9b4c16e204eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=4>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.pow(2,2) #testing this to make sure tf.math.pow is the right thing to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09a7ec98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=48618.504>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(4.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.math.exp(tf.math.pow(x-1, 2))\n",
    "dy_dx = tape.gradient(y,x)\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69449e",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Use `tensorflow` to find the gradient of the function $f: \\mathbb{R}^2 \\to \\mathbb{R},\\ f(x,y) = (x-1)^2 - y^2+1$ at $(x,y) = (2,3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "928b6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x , y = tf.Variable(2.0) , tf.Variable(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1faee1b9-2375-4561-8cb8-3c6fc054d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape: \n",
    "    z = tf.math.pow(x-1, 2) - tf.math.pow(y, 2) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a50fc0dc-0b20-476d-8d4e-278e6c9480ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=2.0>, <tf.Tensor: shape=(), dtype=float32, numpy=-6.0>]\n"
     ]
    }
   ],
   "source": [
    "gradients = tape.gradient(z , [x , y])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad35702-a856-4729-95a0-58037db07c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
